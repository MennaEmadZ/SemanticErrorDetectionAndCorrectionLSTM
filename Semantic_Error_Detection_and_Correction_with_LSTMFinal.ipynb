{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Semantic_Error_Detection_and_Correction_with_LSTMFinal.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2l-gG-c4TSQZ",
        "DSYVN1yGRqhN"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqAOYBEvkBhy"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import re"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxXT2iRZoyiG"
      },
      "source": [
        "#functions dictionary \n",
        "def smart_open(fname, mode = 'r'):\n",
        "    if fname.endswith('.gz'):\n",
        "        import gzip\n",
        "        # Using max compression (9) by default seems to be slow.                                \n",
        "        # Let's try using the fastest.                                                          \n",
        "        return gzip.open(fname, mode, 1)\n",
        "    else:\n",
        "        return open(fname, mode)\n",
        "\n",
        "def tokanizer(list):\n",
        "  tokens =[]\n",
        "  for s in list:\n",
        "      for i in s:\n",
        "          tokens.append(i.split(' '))\n",
        "  return tokens\n",
        "\n",
        "def randint(b, a=0):\n",
        "    return random.randint(a,b)\n",
        "\n",
        "def uniq(seq, idfun=None):\n",
        "    # order preserving                                                                          \n",
        "    if idfun is None:\n",
        "        def idfun(x): return x\n",
        "    seen = {}\n",
        "    result = []\n",
        "    for item in seq:\n",
        "        marker = idfun(item)\n",
        "        # in old Python versions:                                                               \n",
        "        # if seen.has_key(marker)                                                               \n",
        "        # but in new ones:                                                                      \n",
        "        if marker in seen: continue\n",
        "        seen[marker] = 1\n",
        "        result.append(item)\n",
        "    return result\n",
        "    \n",
        "def sort_dict(myDict, byValue=False, reverse=False):\n",
        "    if byValue:\n",
        "        items = myDict.items()\n",
        "        items.sort(key = operator.itemgetter(1), reverse=reverse)\n",
        "    else:\n",
        "        items = sorted(myDict.items())\n",
        "    return items\n",
        "\n",
        "def max_dict(myDict, byValue=False):\n",
        "    if byValue:\n",
        "        skey=lambda x:x[1]\n",
        "    else:\n",
        "        skey=lambda x:x[0]\n",
        "    return max(myDict.items(), key=skey)\n",
        "\n",
        "\n",
        "def min_dict(myDict, byValue=False):\n",
        "    if byValue:\n",
        "        skey=lambda x:x[1]\n",
        "    else:\n",
        "        skey=lambda x:x[0]\n",
        "    return min(myDict.items(), key=skey)\n",
        "\n",
        "def paragraphs(lines, is_separator=lambda x : x == '\\n', joiner=''.join):\n",
        "    paragraph = []\n",
        "    for line in lines:\n",
        "        if is_separator(line):\n",
        "            if paragraph:\n",
        "                yield joiner(paragraph)\n",
        "                paragraph = []\n",
        "        else:\n",
        "            paragraph.append(line)\n",
        "    if paragraph:\n",
        "        yield joiner(paragraph)\n",
        "\n",
        "def isASCII(word):\n",
        "    try:\n",
        "        word = word.decode(\"ascii\")\n",
        "        return True\n",
        "    except UnicodeEncodeError :\n",
        "        return False\n",
        "    except UnicodeDecodeError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def intersect(x, y):\n",
        "    return [z for z in x if z in y]\n",
        "\n",
        "# Mapping Windows CP1252 Gremlins to Unicode\n",
        "# from http://effbot.org/zone/unicode-gremlins.htm\n",
        "cp1252 = {\n",
        "    # from http://www.microsoft.com/typography/unicode/1252.htm\n",
        "    u\"\\x80\": u\"\\u20AC\", # EURO SIGN\n",
        "    u\"\\x82\": u\"\\u201A\", # SINGLE LOW-9 QUOTATION MARK\n",
        "    u\"\\x83\": u\"\\u0192\", # LATIN SMALL LETTER F WITH HOOK\n",
        "    u\"\\x84\": u\"\\u201E\", # DOUBLE LOW-9 QUOTATION MARK\n",
        "    u\"\\x85\": u\"\\u2026\", # HORIZONTAL ELLIPSIS\n",
        "    u\"\\x86\": u\"\\u2020\", # DAGGER\n",
        "    u\"\\x87\": u\"\\u2021\", # DOUBLE DAGGER\n",
        "    u\"\\x88\": u\"\\u02C6\", # MODIFIER LETTER CIRCUMFLEX ACCENT\n",
        "    u\"\\x89\": u\"\\u2030\", # PER MILLE SIGN\n",
        "    u\"\\x8A\": u\"\\u0160\", # LATIN CAPITAL LETTER S WITH CARON\n",
        "    u\"\\x8B\": u\"\\u2039\", # SINGLE LEFT-POINTING ANGLE QUOTATION MARK\n",
        "    u\"\\x8C\": u\"\\u0152\", # LATIN CAPITAL LIGATURE OE\n",
        "    u\"\\x8E\": u\"\\u017D\", # LATIN CAPITAL LETTER Z WITH CARON\n",
        "    u\"\\x91\": u\"\\u2018\", # LEFT SINGLE QUOTATION MARK\n",
        "    u\"\\x92\": u\"\\u2019\", # RIGHT SINGLE QUOTATION MARK\n",
        "    u\"\\x93\": u\"\\u201C\", # LEFT DOUBLE QUOTATION MARK\n",
        "    u\"\\x94\": u\"\\u201D\", # RIGHT DOUBLE QUOTATION MARK\n",
        "    u\"\\x95\": u\"\\u2022\", # BULLET\n",
        "    u\"\\x96\": u\"\\u2013\", # EN DASH\n",
        "    u\"\\x97\": u\"\\u2014\", # EM DASH\n",
        "    u\"\\x98\": u\"\\u02DC\", # SMALL TILDE\n",
        "    u\"\\x99\": u\"\\u2122\", # TRADE MARK SIGN\n",
        "    u\"\\x9A\": u\"\\u0161\", # LATIN SMALL LETTER S WITH CARON\n",
        "    u\"\\x9B\": u\"\\u203A\", # SINGLE RIGHT-POINTING ANGLE QUOTATION MARK\n",
        "    u\"\\x9C\": u\"\\u0153\", # LATIN SMALL LIGATURE OE\n",
        "    u\"\\x9E\": u\"\\u017E\", # LATIN SMALL LETTER Z WITH CARON\n",
        "    u\"\\x9F\": u\"\\u0178\", # LATIN CAPITAL LETTER Y WITH DIAERESIS\n",
        "}\n",
        "\n",
        "def fix_cp1252codes(text):\n",
        "    # map cp1252 gremlins to real unicode characters\n",
        "    if re.search(u\"[\\x80-\\x9f]\", text):\n",
        "        def fixup(m):\n",
        "            s = m.group(0)\n",
        "            return cp1252.get(s, s)\n",
        "        if isinstance(text, type(\"\")):\n",
        "            # make sure we have a unicode string\n",
        "            text = unicode(text, \"iso-8859-1\")\n",
        "        text = re.sub(u\"[\\x80-\\x9f]\", fixup, text)\n",
        "    return text\n",
        "\n",
        "def clean_utf8(text):\n",
        "    return filter(lambda x : x > '\\x1f' and x < '\\x7f', text)\n",
        "\n",
        "def pairs(iterable, overlapping=False):\n",
        "    iterator = iterable.__iter__()\n",
        "    token = iterator.next()\n",
        "    i = 0\n",
        "    for lookahead in iterator:\n",
        "        if overlapping or i % 2 == 0: \n",
        "            yield (token, lookahead)\n",
        "        token = lookahead\n",
        "        i += 1\n",
        "    if i % 2 == 0:\n",
        "        yield (token, None)\n",
        "\n",
        "def frange(start, end=None, inc=None):\n",
        "    \"A range function, that does accept float increments...\"\n",
        "\n",
        "    if end == None:\n",
        "        end = start + 0.0\n",
        "        start = 0.0\n",
        "\n",
        "    if inc == None:\n",
        "        inc = 1.0\n",
        "\n",
        "    L = []\n",
        "    while 1:\n",
        "        next = start + len(L) * inc\n",
        "        if inc > 0 and next >= end:\n",
        "            break\n",
        "        elif inc < 0 and next <= end:\n",
        "            break\n",
        "        L.append(next)\n",
        "        \n",
        "    return L\n",
        "\n",
        "def softmax(values):\n",
        "    a = max(values)\n",
        "    Z = 0.0\n",
        "    for v in values:\n",
        "        Z += math.exp(v - a)\n",
        "    sm = [math.exp(v-a) / Z for v in values]\n",
        "    return sm"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mln7ndV2YFsS"
      },
      "source": [
        "import sys\n",
        "from getopt import getopt\n",
        "\n",
        "def load_annotation(gold_file):\n",
        "    source_sentences = []\n",
        "    gold_edits = []\n",
        "    fgold = smart_open(gold_file, 'r')\n",
        "    puffer = fgold.read()\n",
        "    fgold.close()\n",
        "    #puffer = puffer.decode('utf8')\n",
        "    for item in paragraphs(puffer.splitlines(True)):\n",
        "        item = item.splitlines(False)\n",
        "        sentence = [line[2:].strip() for line in item if line.startswith('S ')]\n",
        "        assert sentence != []\n",
        "        annotations = {}\n",
        "        for line in item[1:]:\n",
        "            if line.startswith('I ') or line.startswith('S '):\n",
        "                continue\n",
        "            assert line.startswith('A ')\n",
        "            line = line[2:]\n",
        "            fields = line.split('|||')\n",
        "            #print(fields)\n",
        "            start_offset = int(fields[0].split()[0])\n",
        "            end_offset = int(fields[0].split()[1])\n",
        "            etype = fields[1]\n",
        "            if etype == 'noop':\n",
        "                start_offset = -1\n",
        "                end_offset = -1\n",
        "            corrections =  [c.strip() if c != '-NONE-' else '' for c in fields[2].split('||')]\n",
        "            # NOTE: start and end are *token* offsets\n",
        "            original = ' '.join(' '.join(sentence).split()[start_offset:end_offset])\n",
        "            if len(fields) > 4:\n",
        "              annotator = int(fields[5])\n",
        "            if annotator not in annotations.keys():\n",
        "                annotations[annotator] = []\n",
        "            annotations[annotator].append((start_offset, end_offset, original, corrections))\n",
        "        tok_offset = 0\n",
        "        for this_sentence in sentence:\n",
        "            tok_offset += len(this_sentence.split())\n",
        "            source_sentences.append(this_sentence)\n",
        "            this_edits = {}\n",
        "            for annotator, annotation in annotations.items():\n",
        "                this_edits[annotator] = [edit for edit in annotation if edit[0] <= tok_offset and edit[1] <= tok_offset and edit[0] >= 0 and edit[1] >= 0]\n",
        "            if len(this_edits) == 0:\n",
        "                this_edits[0] = []\n",
        "            gold_edits.append(this_edits)\n",
        "    return (source_sentences, gold_edits)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l-gG-c4TSQZ"
      },
      "source": [
        "# **Train Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFaSDz1tZ1W3"
      },
      "source": [
        "m2_list_sentance=[]\n",
        "m2_list_operation=[]\n",
        "\n",
        "m2_list_sentance.append(load_annotation('/content/QALB-Train2014.m2')[0])\n",
        "m2_list_operation.append(load_annotation('/content/QALB-Train2014.m2')[1])\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW943sQAwCHy",
        "outputId": "81c7108f-457a-45fd-efc6-0065e054454d"
      },
      "source": [
        "tokens =[]\n",
        "i=0\n",
        "for i in m2_list_sentance:\n",
        "    tokens = tokanizer(m2_list_sentance)\n",
        "print(tokens[0])\n",
        "len(tokens)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['الى', 'التعليق', 'رقم', '2', 'اكيد', 'ان', 'لحكام', 'العرب', 'والمسلمين', 'مسؤولية', 'يتمثل', 'ادناها', 'في', 'استدعاء', 'السفراء', 'في', 'الصين', 'للتشاور', '.', 'فليتهم', 'يفعلونها', 'ولو', 'مرة', '.', 'ولنا', 'نحن', 'كشعوب', 'مسؤولية', 'كذالك', 'تتمثل', 'في', 'مساندة', 'اخواننا', 'في', 'الصين', 'بمقاطعة', 'البضائع', 'الصينينة', 'وليتنا', 'نفعلها', 'ولو', 'ثلاتة', 'اشهر', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19411"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuR3ABbfpTEx"
      },
      "source": [
        "def extract(operation):\n",
        "  input=[]\n",
        "  correct=[]\n",
        "\n",
        "  for i in range(len(operation)):\n",
        "          if (operation[i][0]!=operation[i][1]):  \n",
        "              input.append(operation[i][2])\n",
        "              correct.append(operation[i][3][0])\n",
        "  return (input , correct)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEnxOtQIwcBw"
      },
      "source": [
        "list_correction=[]\n",
        "list_input=[]\n",
        "for i in range(len(tokens)):\n",
        "  list_input.append((extract(m2_list_operation[0][i][0])[0]))\n",
        "  list_correction.append((extract(m2_list_operation[0][i][0])[1]))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atsG_bs_NWfo"
      },
      "source": [
        "# Preprocessing regular expressions\n",
        "import re\n",
        "def process(sent):\n",
        "    # sent=sent.lower()\n",
        "    # sent=re.sub(r'[^0-9]','',sent)\n",
        "\n",
        "    #sent=sent.replace(' ','\\n')\n",
        "    sent=sent.replace('ـ','')\n",
        "    sent=sent.replace('-','')\n",
        "    sent=sent.replace(')','')\n",
        "    sent=sent.replace('(','')\n",
        "    sent=sent.replace('*','')\n",
        "    sent=sent.replace(',','')\n",
        "    sent=sent.replace('/','')\n",
        "    sent=sent.replace('&','')\n",
        "    sent=sent.replace('amp','')\n",
        "    sent=sent.replace('_','')\n",
        "    sent=sent.replace('ّ','')\n",
        "    sent=sent.replace('ً','')\n",
        "    sent=sent.replace('%','')\n",
        "    sent=sent.replace(';','')\n",
        "    sent=sent.replace('ِ','')\n",
        "    sent=sent.replace('@','')\n",
        "    sent=sent.replace('ګ','')\n",
        "    sent=sent.replace( 'ٍ','')\n",
        "    sent=sent.replace('٪','')\n",
        "    sent=sent.replace('ٱ','')\n",
        "    sent=sent.replace('ُ','')\n",
        "    sent=sent.replace(':','')\n",
        "    sent=sent.replace('؛','')\n",
        "    sent=sent.replace('[','')\n",
        "    sent=sent.replace(']','')\n",
        "    sent=sent.replace('=','')\n",
        "    sent=sent.replace('^','')\n",
        "    sent=sent.replace('?','')\n",
        "    sent=sent.replace('{','')\n",
        "    sent=sent.replace('}','')\n",
        "    sent=sent.replace('g','')\n",
        "    sent=sent.replace('t','')\n",
        "    sent=sent.replace('a','')\n",
        "    sent=sent.replace('b','')\n",
        "    sent=sent.replace('c','')\n",
        "    sent=sent.replace('d','')\n",
        "    sent=sent.replace('s','')\n",
        "    sent=sent.replace('f','')\n",
        "    sent=sent.replace('h','')\n",
        "    sent=sent.replace('j','')\n",
        "    sent=sent.replace('k','')\n",
        "    sent=sent.replace('l','')\n",
        "    sent=sent.replace('q','')\n",
        "    sent=sent.replace('t','')\n",
        "    sent=sent.replace('a','')\n",
        "    sent=sent.replace('b','')\n",
        "    sent=sent.replace('c','')\n",
        "    sent=sent.replace('d','')\n",
        "    sent=sent.replace('s','')\n",
        "    sent=sent.replace('f','')\n",
        "    sent=sent.replace('h','')\n",
        "    sent=sent.replace('j','')\n",
        "    sent=sent.replace('k','')\n",
        "    sent=sent.replace('l','')\n",
        "    sent=sent.replace('q','')\n",
        "    sent=sent.replace('w','')\n",
        "    sent=sent.replace('e','')\n",
        "    sent=sent.replace('r','')\n",
        "    sent=sent.replace('y','')\n",
        "    sent=sent.replace('u','')\n",
        "    sent=sent.replace('i','')\n",
        "    sent=sent.replace('o','')\n",
        "    sent=sent.replace('p','')\n",
        "    sent=sent.replace('z','')\n",
        "    sent=sent.replace('x','')\n",
        "    sent=sent.replace('v','')\n",
        "    sent=sent.replace('n','')\n",
        "    sent=sent.replace('m','')\n",
        "    sent=sent.replace('\\\\','')\n",
        "    sent=sent.replace('$','')\n",
        "    sent=sent.replace('`','')\n",
        "    sent=sent.replace('~','')\n",
        "    sent=sent.replace('|','')\n",
        "    sent=sent.replace('گ','')\n",
        "    sent=sent.replace(\"'\",'')\n",
        "    # sent=sent.replace('ُ','')\n",
        "    # sent=sent.replace(':','')\n",
        "    # sent=sent.replace('ُ','')\n",
        "    # sent=sent.replace(':','')\n",
        "    # sent=sent.replace('ُ','')\n",
        "    # sent=sent.replace(':','')\n",
        "    # sent=sent.replace('ُ','')\n",
        "    # sent=sent.replace(':','')\n",
        "    # sent=sent.replace('ُ','')\n",
        "    # sent=sent.replace(':','')\n",
        "\n",
        "    return sent   "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "T_wvI6i8NjzZ",
        "outputId": "12387d59-a9a6-419e-a0ab-83dc7563a544"
      },
      "source": [
        "#applying the preproccesing &separating the words again\n",
        "input_sents=[]\n",
        "for i in range(len(list_input)):\n",
        "    input_sent =[process(x) for x in list_input[i]]\n",
        "    for line in input_sent :\n",
        "            input_sents.append(line)\n",
        "#print(\"\\n\".join(target[:4]))\n",
        "print(\"Number of items:\",len(input_sents))\n",
        "input_sents[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of items: 207479\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'الى'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erpWZT8INxpX",
        "outputId": "12804770-f952-4b61-eb4d-911e132ad117"
      },
      "source": [
        "#applying the preproccesing &separating the words again\n",
        "target_sent=[]\n",
        "for i in range(len(list_correction)):\n",
        "    linesout =[process(x) for x in list_correction[i]]\n",
        "    for line in linesout :\n",
        "            target_sent.append(line)    \n",
        "#print(\"\\n\".join(target[:4]))\n",
        "print(\"Number of items:\",len(target_sent))\n",
        "target_sent[0:4]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of items: 207479\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['إلى', 'أكيد', 'أن', 'للحكام']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iovrSk7xNga-",
        "outputId": "539fcd7d-9955-461f-c505-f2a9c5b2fd7d"
      },
      "source": [
        "#char indexing\n",
        "char_set = list('+#’ \"‘ی،.؟!يـابتةثجپحخدذرزسشصضطظعڤغفقكلمنهوىيءآۆأؤإڕئک١۱٢٣٤٥٦٧٨٩۹٠0123456789')\n",
        "char2int = { char_set[x]:x for x in range(len(char_set)) }\n",
        "int2char = { char2int[x]:x for x in char_set }\n",
        "print(char2int)\n",
        "print(int2char)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'+': 0, '#': 1, '’': 2, ' ': 3, '\"': 4, '‘': 5, 'ی': 6, '،': 7, '.': 8, '؟': 9, '!': 10, 'ي': 44, 'ـ': 12, 'ا': 13, 'ب': 14, 'ت': 15, 'ة': 16, 'ث': 17, 'ج': 18, 'پ': 19, 'ح': 20, 'خ': 21, 'د': 22, 'ذ': 23, 'ر': 24, 'ز': 25, 'س': 26, 'ش': 27, 'ص': 28, 'ض': 29, 'ط': 30, 'ظ': 31, 'ع': 32, 'ڤ': 33, 'غ': 34, 'ف': 35, 'ق': 36, 'ك': 37, 'ل': 38, 'م': 39, 'ن': 40, 'ه': 41, 'و': 42, 'ى': 43, 'ء': 45, 'آ': 46, 'ۆ': 47, 'أ': 48, 'ؤ': 49, 'إ': 50, 'ڕ': 51, 'ئ': 52, 'ک': 53, '١': 54, '۱': 55, '٢': 56, '٣': 57, '٤': 58, '٥': 59, '٦': 60, '٧': 61, '٨': 62, '٩': 63, '۹': 64, '٠': 65, '0': 66, '1': 67, '2': 68, '3': 69, '4': 70, '5': 71, '6': 72, '7': 73, '8': 74, '9': 75}\n",
            "{0: '+', 1: '#', 2: '’', 3: ' ', 4: '\"', 5: '‘', 6: 'ی', 7: '،', 8: '.', 9: '؟', 10: '!', 44: 'ي', 12: 'ـ', 13: 'ا', 14: 'ب', 15: 'ت', 16: 'ة', 17: 'ث', 18: 'ج', 19: 'پ', 20: 'ح', 21: 'خ', 22: 'د', 23: 'ذ', 24: 'ر', 25: 'ز', 26: 'س', 27: 'ش', 28: 'ص', 29: 'ض', 30: 'ط', 31: 'ظ', 32: 'ع', 33: 'ڤ', 34: 'غ', 35: 'ف', 36: 'ق', 37: 'ك', 38: 'ل', 39: 'م', 40: 'ن', 41: 'ه', 42: 'و', 43: 'ى', 45: 'ء', 46: 'آ', 47: 'ۆ', 48: 'أ', 49: 'ؤ', 50: 'إ', 51: 'ڕ', 52: 'ئ', 53: 'ک', 54: '١', 55: '۱', 56: '٢', 57: '٣', 58: '٤', 59: '٥', 60: '٦', 61: '٧', 62: '٨', 63: '٩', 64: '۹', 65: '٠', 66: '0', 67: '1', 68: '2', 69: '3', 70: '4', 71: '5', 72: '6', 73: '7', 74: '8', 75: '9'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2_47_kFN5H3",
        "outputId": "2f92efdf-8fc8-4130-ca1c-d64d99df427e"
      },
      "source": [
        "#adding additional codes that then used in creating the dataset\n",
        "count = len(char_set)\n",
        "codes = [\"\\t\",\"\\n\"]\n",
        "for i in range(len(codes)):\n",
        "    code = codes[i] #i=0,code=\\t\n",
        "    char2int[code]=count\n",
        "    int2char[count]=code\n",
        "    count+=1\n",
        "print(char2int)\n",
        "print(int2char)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'+': 0, '#': 1, '’': 2, ' ': 3, '\"': 4, '‘': 5, 'ی': 6, '،': 7, '.': 8, '؟': 9, '!': 10, 'ي': 44, 'ـ': 12, 'ا': 13, 'ب': 14, 'ت': 15, 'ة': 16, 'ث': 17, 'ج': 18, 'پ': 19, 'ح': 20, 'خ': 21, 'د': 22, 'ذ': 23, 'ر': 24, 'ز': 25, 'س': 26, 'ش': 27, 'ص': 28, 'ض': 29, 'ط': 30, 'ظ': 31, 'ع': 32, 'ڤ': 33, 'غ': 34, 'ف': 35, 'ق': 36, 'ك': 37, 'ل': 38, 'م': 39, 'ن': 40, 'ه': 41, 'و': 42, 'ى': 43, 'ء': 45, 'آ': 46, 'ۆ': 47, 'أ': 48, 'ؤ': 49, 'إ': 50, 'ڕ': 51, 'ئ': 52, 'ک': 53, '١': 54, '۱': 55, '٢': 56, '٣': 57, '٤': 58, '٥': 59, '٦': 60, '٧': 61, '٨': 62, '٩': 63, '۹': 64, '٠': 65, '0': 66, '1': 67, '2': 68, '3': 69, '4': 70, '5': 71, '6': 72, '7': 73, '8': 74, '9': 75, '\\t': 76, '\\n': 77}\n",
            "{0: '+', 1: '#', 2: '’', 3: ' ', 4: '\"', 5: '‘', 6: 'ی', 7: '،', 8: '.', 9: '؟', 10: '!', 44: 'ي', 12: 'ـ', 13: 'ا', 14: 'ب', 15: 'ت', 16: 'ة', 17: 'ث', 18: 'ج', 19: 'پ', 20: 'ح', 21: 'خ', 22: 'د', 23: 'ذ', 24: 'ر', 25: 'ز', 26: 'س', 27: 'ش', 28: 'ص', 29: 'ض', 30: 'ط', 31: 'ظ', 32: 'ع', 33: 'ڤ', 34: 'غ', 35: 'ف', 36: 'ق', 37: 'ك', 38: 'ل', 39: 'م', 40: 'ن', 41: 'ه', 42: 'و', 43: 'ى', 45: 'ء', 46: 'آ', 47: 'ۆ', 48: 'أ', 49: 'ؤ', 50: 'إ', 51: 'ڕ', 52: 'ئ', 53: 'ک', 54: '١', 55: '۱', 56: '٢', 57: '٣', 58: '٤', 59: '٥', 60: '٦', 61: '٧', 62: '٨', 63: '٩', 64: '۹', 65: '٠', 66: '0', 67: '1', 68: '2', 69: '3', 70: '4', 71: '5', 72: '6', 73: '7', 74: '8', 75: '9', 76: '\\t', 77: '\\n'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeTzxCPVrKe9",
        "outputId": "ef6f04b8-ebaa-4e8f-c720-244bef0b56a9"
      },
      "source": [
        "# create  dataset\n",
        "target_sents = [] \n",
        "\n",
        "for line in target_sent:\n",
        "        output = '\\t' + line + '\\n'\n",
        "        target_sents.append(output)\n",
        "\n",
        "print(target_sents[:4])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\tإلى\\n', '\\tأكيد\\n', '\\tأن\\n', '\\tللحكام\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCeeiKLKreZI",
        "outputId": "9707c9eb-e778-4651-e39d-7f7c5fb8e895"
      },
      "source": [
        "print(\"LEN OF input_sents:\",len(input_sents))\n",
        "print(\"LEN OF target_sents:\",len(target_sents))\n",
        "print(\"input_texts\",input_sents[:10])\n",
        "print(\"target_texts\",target_sents[:10])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LEN OF input_sents: 207479\n",
            "LEN OF target_sents: 207479\n",
            "input_texts ['الى', 'اكيد', 'ان', 'لحكام', 'ادناها', '.', 'كذالك', 'اخواننا', 'الصينينة', 'ثلاتة']\n",
            "target_texts ['\\tإلى\\n', '\\tأكيد\\n', '\\tأن\\n', '\\tللحكام\\n', '\\tأدناها\\n', '\\t!\\n', '\\tكذلك\\n', '\\tإخواننا\\n', '\\tالصينية\\n', '\\tثلاثة\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y99WPKvh00HE",
        "outputId": "de9d08c7-ac28-43b9-f7a5-754c37f031ae"
      },
      "source": [
        "max_enc_len = max([len(x) for x in input_sents])\n",
        "max_dec_len = max([len(x) for x in target_sents])\n",
        "\n",
        "print(\"Max Enc Len:\",max_enc_len)\n",
        "print(\"Max Dec Len:\",max_dec_len)\n",
        "\n",
        "num_samples = len(input_sents)\n",
        "\n",
        "encoderInput_data = np.zeros( (num_samples , max_enc_len , len(char_set)),dtype='float32' )\n",
        "decoderInput_data = np.zeros( (num_samples , max_dec_len , len(char_set)+2),dtype='float32' )\n",
        "decoderTarget_data = np.zeros( (num_samples , max_dec_len , len(char_set)+2),dtype='float32' )\n",
        "\n",
        "print(\"CREATED ZERO VECTORS\")\n",
        "print(encoderInput_data.shape)\n",
        "\n",
        "#to cover the enumration issue \n",
        "input_sents_enc = [] \n",
        "target_sents_dec = [] \n",
        "\n",
        "for line in input_sents:\n",
        "        input_sents_enc.append(line)\n",
        "\n",
        "for line in target_sents:\n",
        "        target_sents_dec.append(line)\n",
        "\n",
        "print(\"input_sents_enc\",len(input_sents_enc))\n",
        "print(\"target_sents_dec\",len(target_sents_dec))\n",
        "  \n",
        "\n",
        "for i,(input_sents_enc,target_sents_dec) in enumerate(zip(input_sents_enc,target_sents_dec)): #enumerate : keep a count of iterations\n",
        "   \n",
        "    for t,char in enumerate(input_sents_enc):\n",
        "        encoderInput_data[ i , t , char2int[char] ] = 1  #i=index of word, t=index of char, cha2int[char]=index of char in our char dict \n",
        "                                                            #assign all of that to 1 in encoder\n",
        "    for t,char in enumerate(target_sents_dec):\n",
        "        decoderInput_data[ i, t , char2int[char] ] = 1\n",
        "        if t > 0 :\n",
        "            decoderTarget_data[ i , t-1 , char2int[char] ] = 1 #as the word starts with \\t and end with \\n\n",
        "   \n",
        "print(\"Encoding COMPLETED...\")  "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Enc Len: 53\n",
            "Max Dec Len: 47\n",
            "CREATED ZERO VECTORS\n",
            "(207479, 53, 76)\n",
            "input_sents_enc 207479\n",
            "target_sents_dec 207479\n",
            "Encoding COMPLETED...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSYVN1yGRqhN"
      },
      "source": [
        "# **Test Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbhv6XrZQ-x6"
      },
      "source": [
        "m2_list_sentance_test=[]\n",
        "m2_list_operation_test=[]\n",
        "\n",
        "m2_list_sentance_test.append(load_annotation('/content/QALB-Test2014.m2')[0])\n",
        "m2_list_operation_test.append(load_annotation('/content/QALB-Test2014.m2')[1])\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZAtO_btR9QV",
        "outputId": "b1815930-7de8-42ce-c050-e02fc6821ed0"
      },
      "source": [
        "tokensTest =[]\n",
        "i=0\n",
        "for i in m2_list_sentance_test:\n",
        "    tokensTest = tokanizer(m2_list_sentance_test)\n",
        "print(tokensTest[3])\n",
        "len(tokensTest)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['تعجبني', 'المظاهرات', 'في', 'حمص', 'هنا', 'وهناك', 'مثل', 'ما', 'راينا', 'في', 'دير', 'بعلبة', 'طبعا', 'كشف', 'النظام', 'السوري', 'عن', 'وجود', 'اماكن', 'للتعذيب', 'و', 'للتقطيع', 'طبعا', 'بالتاكيد', 'هذه', 'لمن', 'يفتح', 'محله', 'او', 'يجلس', 'في', 'بيته', 'ولا', 'ينزل', 'لمظاهرة', 'او', 'لمن', 'يقول', 'عاش', 'الوطن', 'ابيا', 'سالما', 'فبالفعل', 'واسفاه', 'على', 'من', 'تبع', 'هذه', 'الفورة', 'وصدقها', 'و', 'ساند', 'اذيالها']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "968"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RayeIai4Tx1n"
      },
      "source": [
        "test_correction=[]\n",
        "test_input=[]\n",
        "for i in range(len(tokensTest)):\n",
        "  test_input.append((extract(m2_list_operation_test[0][i][0])[0]))\n",
        "  test_correction.append((extract(m2_list_operation_test[0][i][0])[1]))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sDmBV_tkHPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1343f296-3fac-400d-d104-8accd487a686"
      },
      "source": [
        "#applying the preproccesing &separating the words again\n",
        "test_x=[]\n",
        "for i in range(len(test_input)):\n",
        "    input_sent =[process(x) for x in test_input[i]]\n",
        "    for line in input_sent :\n",
        "            test_x.append(line)\n",
        "#print(\"\\n\".join(target[:4]))\n",
        "print(\"Number of items:\",len(test_x))\n",
        "test_x[0:4]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of items: 10690\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['لا زال', 'الشبيحه', 'ان', 'ارواح']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rITesQ5OkMVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83533775-e764-47cc-e715-e4358c20f9f6"
      },
      "source": [
        "#applying the preproccesing &separating the words again\n",
        "target_y=[]\n",
        "for i in range(len(test_correction)):\n",
        "    linesout =[process(x) for x in test_correction[i]]\n",
        "    for line in linesout :\n",
        "            target_y.append(line)    \n",
        "#print(\"\\n\".join(target[:4]))\n",
        "print(\"Number of items:\",len(target_y))\n",
        "target_y[0:4]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of items: 10690\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['لازال', 'الشبيحة', 'أن', 'أرواح']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w38TYZkodFKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c287c1f-3503-4ee7-ede7-793bda78f2af"
      },
      "source": [
        "# create  dataset\n",
        "test_y= [] \n",
        "\n",
        "for line in target_y:\n",
        "        output = '\\t' + line + '\\n'\n",
        "        test_y.append(output)\n",
        "\n",
        "print(test_y[:4])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\tلازال\\n', '\\tالشبيحة\\n', '\\tأن\\n', '\\tأرواح\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TijPi3ygSSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc0b8d3-201a-40f3-8a0f-9135742ecbd5"
      },
      "source": [
        "print(\"LEN OF input_sents:\",len(test_x))\n",
        "print(\"LEN OF target_sents:\",len(test_y))\n",
        "print(\"input_texts\",test_x[:10])\n",
        "print(\"target_texts\",test_y[:10])\n",
        "   "
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LEN OF input_sents: 10690\n",
            "LEN OF target_sents: 10690\n",
            "input_texts ['لا زال', 'الشبيحه', 'ان', 'ارواح', 'اقل', 'كلفه', 'اذا', 'المعادله', 'المهينه', 'ان']\n",
            "target_texts ['\\tلازال\\n', '\\tالشبيحة\\n', '\\tأن\\n', '\\tأرواح\\n', '\\tأقل\\n', '\\tكلفة\\n', '\\tإذا\\n', '\\tالمعادلة\\n', '\\tالمهينة\\n', '\\tأن\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU0T8HwZgqP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b896d9a-f309-49b7-8747-c709ade81d56"
      },
      "source": [
        "\n",
        "max_enc_len_test = max([len(x) for x in test_x])\n",
        "\n",
        "max_dec_len_test = max([len(x) for x in test_y])\n",
        "\n",
        "print(\"max_enc_len_test:\",max_enc_len_test)\n",
        "print(\"max_dec_len_test:\",max_dec_len_test)\n",
        "\n",
        "num_samples = len(test_x)\n",
        "\n",
        "encoderInput_datax = np.zeros( (num_samples , max_enc_len_test , len(char_set)),dtype='float32' )\n",
        "decoderInput_datay = np.zeros( (num_samples , max_dec_len_test , len(char_set)+2),dtype='float32' )\n",
        "decoderTarget_datay = np.zeros( (num_samples , max_dec_len_test , len(char_set)+2),dtype='float32' )\n",
        "print(\"CREATED ZERO VECTORS\")\n",
        "\n",
        "\n",
        "#to cover the enumration issue \n",
        "test_x_enc = [] \n",
        "test_y_dec = [] \n",
        "\n",
        "for line in test_x:\n",
        "        test_x_enc.append(line)\n",
        "\n",
        "for line in test_y:\n",
        "        test_y_dec.append(line)\n",
        "\n",
        "print(\"input_sents_enc\",len(test_x_enc))\n",
        "print(\"test_y_dec\",len(test_y_dec))\n",
        "  \n",
        "\n",
        "for i,(test_x_enc,test_y_dec) in enumerate(zip(test_x_enc,test_y_dec)): #enumerate : keep a count of iterations\n",
        "   \n",
        "    for t,char in enumerate(test_x_enc):\n",
        "        encoderInput_datax[ i , t , char2int[char] ] = 1  #i=index of word, t=index of char, cha2int[char]=index of char in our char dict \n",
        "                                                            #assign all of that to 1 in encoder\n",
        "    for t,char in enumerate(test_y_dec):\n",
        "        decoderInput_datay[ i, t , char2int[char] ] = 1\n",
        "        if t > 0 :\n",
        "            decoderTarget_datay[ i , t-1 , char2int[char] ] = 1 #as the word starts with \\t and end with \\n\n",
        "   \n",
        "print(\"Encoding COMPLETED...\")  \n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_enc_len_test: 33\n",
            "max_dec_len_test: 35\n",
            "CREATED ZERO VECTORS\n",
            "input_sents_enc 10690\n",
            "test_y_dec 10690\n",
            "Encoding COMPLETED...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmMl6mka0cLQ"
      },
      "source": [
        "# **THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrWvF-3bkxSC"
      },
      "source": [
        "#beginning of the model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input,LSTM,Dense"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIygmV9Cky-u"
      },
      "source": [
        "#intializing the model\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "latent_dim = 256  #embedding dimentions\n",
        "num_enc_tokens =76\n",
        "num_dec_tokens = 76 + 2 # includes \\n \\t\n",
        "encoder_inputs = Input(shape=(None,num_enc_tokens))\n",
        "\n",
        "#return_state: need to have its cell state initialized with previous time step while the weights are shared\n",
        "encoder = LSTM(latent_dim,return_state=True) \n",
        "encoder_outputs , state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h,state_c]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3usEHeNlk0mS"
      },
      "source": [
        "decoder_inputs = Input(shape=(None,num_dec_tokens))\n",
        "#return sequences return the hidden state output for each input time step.\n",
        "#return state returns the hidden state output and cell state for the last input time step\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
        "decoder_ouputs,_,_ = decoder_lstm(decoder_inputs,initial_state = encoder_states)\n",
        "\n",
        "decoder_dense = Dense(num_dec_tokens, activation='softmax')\n",
        "decoder_ouputs = decoder_dense(decoder_ouputs)\n",
        "\n",
        "model = Model([encoder_inputs,decoder_inputs],decoder_ouputs)\n",
        "model.compile(optimizer='rmsprop',loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdmq1TuUk2W9",
        "outputId": "d382166e-397d-42bf-a498-5a51b804148e"
      },
      "source": [
        "#train the model\n",
        "h=model.fit(\n",
        "         [encoderInput_data,decoderInput_data],decoderTarget_data,\n",
        "         epochs = epochs,\n",
        "          batch_size = batch_size,\n",
        "          )\n",
        "model.save('s2s.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1621/1621 [==============================] - 34s 17ms/step - loss: 0.4299\n",
            "Epoch 2/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.3577\n",
            "Epoch 3/200\n",
            "1621/1621 [==============================] - 28s 17ms/step - loss: 0.3443\n",
            "Epoch 4/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.3223\n",
            "Epoch 5/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.3035\n",
            "Epoch 6/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.2886\n",
            "Epoch 7/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.2732\n",
            "Epoch 8/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.2592\n",
            "Epoch 9/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.2453\n",
            "Epoch 10/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.2307\n",
            "Epoch 11/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.2174\n",
            "Epoch 12/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.2028\n",
            "Epoch 13/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1910\n",
            "Epoch 14/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1783\n",
            "Epoch 15/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1682\n",
            "Epoch 16/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1597\n",
            "Epoch 17/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1514\n",
            "Epoch 18/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1446\n",
            "Epoch 19/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1378\n",
            "Epoch 20/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1328\n",
            "Epoch 21/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1290\n",
            "Epoch 22/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1240\n",
            "Epoch 23/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1194\n",
            "Epoch 24/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1151\n",
            "Epoch 25/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1128\n",
            "Epoch 26/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1095\n",
            "Epoch 27/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1062\n",
            "Epoch 28/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1040\n",
            "Epoch 29/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.1017\n",
            "Epoch 30/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0998\n",
            "Epoch 31/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0974\n",
            "Epoch 32/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0978\n",
            "Epoch 33/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0953\n",
            "Epoch 34/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0942\n",
            "Epoch 35/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0925\n",
            "Epoch 36/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0903\n",
            "Epoch 37/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0883\n",
            "Epoch 38/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0878\n",
            "Epoch 39/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0857\n",
            "Epoch 40/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0844\n",
            "Epoch 41/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0837\n",
            "Epoch 42/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0825\n",
            "Epoch 43/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0815\n",
            "Epoch 44/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0807\n",
            "Epoch 45/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0796\n",
            "Epoch 46/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0785\n",
            "Epoch 47/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0776\n",
            "Epoch 48/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0773\n",
            "Epoch 49/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0760\n",
            "Epoch 50/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0756\n",
            "Epoch 51/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0743\n",
            "Epoch 52/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0739\n",
            "Epoch 53/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0729\n",
            "Epoch 54/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0725\n",
            "Epoch 55/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0725\n",
            "Epoch 56/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0717\n",
            "Epoch 57/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0711\n",
            "Epoch 58/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0702\n",
            "Epoch 59/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0697\n",
            "Epoch 60/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0692\n",
            "Epoch 61/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0685\n",
            "Epoch 62/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0686\n",
            "Epoch 63/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0684\n",
            "Epoch 64/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0684\n",
            "Epoch 65/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0676\n",
            "Epoch 66/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0668\n",
            "Epoch 67/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0665\n",
            "Epoch 68/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0655\n",
            "Epoch 69/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0659\n",
            "Epoch 70/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0645\n",
            "Epoch 71/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0652\n",
            "Epoch 72/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0641\n",
            "Epoch 73/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0641\n",
            "Epoch 74/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0639\n",
            "Epoch 75/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0630\n",
            "Epoch 76/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0631\n",
            "Epoch 77/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0625\n",
            "Epoch 78/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0622\n",
            "Epoch 79/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0620\n",
            "Epoch 80/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0619\n",
            "Epoch 81/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0615\n",
            "Epoch 82/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0610\n",
            "Epoch 83/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0608\n",
            "Epoch 84/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0602\n",
            "Epoch 85/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0607\n",
            "Epoch 86/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0600\n",
            "Epoch 87/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0597\n",
            "Epoch 88/200\n",
            "1621/1621 [==============================] - 28s 17ms/step - loss: 0.0591\n",
            "Epoch 89/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0587\n",
            "Epoch 90/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0589\n",
            "Epoch 91/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0584\n",
            "Epoch 92/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0584\n",
            "Epoch 93/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0581\n",
            "Epoch 94/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0579\n",
            "Epoch 95/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0596\n",
            "Epoch 96/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0640\n",
            "Epoch 97/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0602\n",
            "Epoch 98/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0585\n",
            "Epoch 99/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0576\n",
            "Epoch 100/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0572\n",
            "Epoch 101/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0572\n",
            "Epoch 102/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0573\n",
            "Epoch 103/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0568\n",
            "Epoch 104/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0569\n",
            "Epoch 105/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0564\n",
            "Epoch 106/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0568\n",
            "Epoch 107/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0557\n",
            "Epoch 108/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0560\n",
            "Epoch 109/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0555\n",
            "Epoch 110/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0549\n",
            "Epoch 111/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0554\n",
            "Epoch 112/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0552\n",
            "Epoch 113/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0544\n",
            "Epoch 114/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0546\n",
            "Epoch 115/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0544\n",
            "Epoch 116/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0542\n",
            "Epoch 117/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0538\n",
            "Epoch 118/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0538\n",
            "Epoch 119/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0536\n",
            "Epoch 120/200\n",
            "1621/1621 [==============================] - 28s 17ms/step - loss: 0.0535\n",
            "Epoch 121/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0535\n",
            "Epoch 122/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0527\n",
            "Epoch 123/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0527\n",
            "Epoch 124/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0521\n",
            "Epoch 125/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0524\n",
            "Epoch 126/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0518\n",
            "Epoch 127/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0523\n",
            "Epoch 128/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0523\n",
            "Epoch 129/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0517\n",
            "Epoch 130/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0512\n",
            "Epoch 131/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0515\n",
            "Epoch 132/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0514\n",
            "Epoch 133/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0508\n",
            "Epoch 134/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0509\n",
            "Epoch 135/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0504\n",
            "Epoch 136/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0506\n",
            "Epoch 137/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0504\n",
            "Epoch 138/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0502\n",
            "Epoch 139/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0500\n",
            "Epoch 140/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0502\n",
            "Epoch 141/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0500\n",
            "Epoch 142/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0497\n",
            "Epoch 143/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0497\n",
            "Epoch 144/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0499\n",
            "Epoch 145/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0495\n",
            "Epoch 146/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0491\n",
            "Epoch 147/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0492\n",
            "Epoch 148/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0489\n",
            "Epoch 149/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0485\n",
            "Epoch 150/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0486\n",
            "Epoch 151/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0487\n",
            "Epoch 152/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0489\n",
            "Epoch 153/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0485\n",
            "Epoch 154/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0482\n",
            "Epoch 155/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0481\n",
            "Epoch 156/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0480\n",
            "Epoch 157/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0481\n",
            "Epoch 158/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0480\n",
            "Epoch 159/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0476\n",
            "Epoch 160/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0466\n",
            "Epoch 161/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0459\n",
            "Epoch 162/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0459\n",
            "Epoch 163/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0452\n",
            "Epoch 164/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0448\n",
            "Epoch 165/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0444\n",
            "Epoch 166/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0440\n",
            "Epoch 167/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0439\n",
            "Epoch 168/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0449\n",
            "Epoch 169/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0432\n",
            "Epoch 170/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0428\n",
            "Epoch 171/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0426\n",
            "Epoch 172/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0425\n",
            "Epoch 173/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0421\n",
            "Epoch 174/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0420\n",
            "Epoch 175/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0423\n",
            "Epoch 176/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0416\n",
            "Epoch 177/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0412\n",
            "Epoch 178/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0413\n",
            "Epoch 179/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0411\n",
            "Epoch 180/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0401\n",
            "Epoch 181/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0402\n",
            "Epoch 182/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0405\n",
            "Epoch 183/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0396\n",
            "Epoch 184/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0392\n",
            "Epoch 185/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0394\n",
            "Epoch 186/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0392\n",
            "Epoch 187/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0392\n",
            "Epoch 188/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0389\n",
            "Epoch 189/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0387\n",
            "Epoch 190/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0388\n",
            "Epoch 191/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0384\n",
            "Epoch 192/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0380\n",
            "Epoch 193/200\n",
            "1621/1621 [==============================] - 27s 17ms/step - loss: 0.0385\n",
            "Epoch 194/200\n",
            "1621/1621 [==============================] - 27s 16ms/step - loss: 0.0383\n",
            "Epoch 195/200\n",
            "1621/1621 [==============================] - 27s 16ms/step - loss: 0.0376\n",
            "Epoch 196/200\n",
            "1621/1621 [==============================] - 27s 16ms/step - loss: 0.0381\n",
            "Epoch 197/200\n",
            "1621/1621 [==============================] - 27s 16ms/step - loss: 0.0377\n",
            "Epoch 198/200\n",
            "1621/1621 [==============================] - 27s 16ms/step - loss: 0.0376\n",
            "Epoch 199/200\n",
            "1621/1621 [==============================] - 27s 16ms/step - loss: 0.0376\n",
            "Epoch 200/200\n",
            "1621/1621 [==============================] - 27s 16ms/step - loss: 0.0373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXehjY0RFa2j"
      },
      "source": [
        "# **Validation Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsF1OPX0uKUT"
      },
      "source": [
        "score = model.evaluate([encoderInput_datax,decoderInput_datay], decoderTarget_datay, batch_size=batch_size, verbose=0)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vduO2UAjNeu",
        "outputId": "96fd5591-bac0-4e13-9aee-fd677ac3c775"
      },
      "source": [
        "print(\"Validation accuracy: \" ,score)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy:  0.7122483849525452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJXFaP4EFEkd"
      },
      "source": [
        "# **LOADING MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U0g5qCGVi-m"
      },
      "source": [
        "from keras.models import load_model\n",
        "Model = load_model('/content/s2s.h5')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gff4JAI7VDLV",
        "outputId": "ca378617-c41b-46f5-d69a-5e935e2fd990"
      },
      "source": [
        "decoder_model = load_model('/content/decoder.h5')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poL5eF50Vi2D",
        "outputId": "20370f0d-a082-445a-bd22-0dd0a37cc020"
      },
      "source": [
        "encoder_model = load_model('/content/encoder.h5')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNVlK2XZFZnq"
      },
      "source": [
        "model.compile(optimizer='rmsprop',loss='categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZnYn31LVYRY"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_dec_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, char2int['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = int2char[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_dec_len):\n",
        "            stop_condition = True\n",
        "            \n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_dec_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr1cx3mMrp5k",
        "outputId": "f4e8600d-c032-43ad-8927-b282e79eaf9f"
      },
      "source": [
        "#model testing\n",
        "\n",
        "#encoder_model = Model(encoder_inputs,encoder_states)\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h,decoder_state_input_c]\n",
        "decoder_outputs,state_h,state_c = decoder_lstm(\n",
        "        decoder_inputs,initial_state = decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h,state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# decoder_model = Model(\n",
        "#     [decoder_inputs] + decoder_states_inputs,\n",
        "#     [decoder_outputs] + decoder_states\n",
        "# ) \n",
        "#encoder_model.save('encoder.h5')\n",
        "#decoder_model.save('decoder.h5')\n",
        "\n",
        "for seq_index in range(100):\n",
        "    input_seq = encoderInput_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Wrong sentence:', input_sents[seq_index])\n",
        "    print('Corrected sentence:', decoded_sentence)\n",
        "    print('Ground Truth:',target_sents[seq_index])"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Wrong sentence: الى\n",
            "Corrected sentence: إلى\n",
            "\n",
            "Ground Truth: \tإلى\n",
            "\n",
            "-\n",
            "Wrong sentence: اكيد\n",
            "Corrected sentence: أكيد\n",
            "\n",
            "Ground Truth: \tأكيد\n",
            "\n",
            "-\n",
            "Wrong sentence: ان\n",
            "Corrected sentence: أن\n",
            "\n",
            "Ground Truth: \tأن\n",
            "\n",
            "-\n",
            "Wrong sentence: لحكام\n",
            "Corrected sentence: لحكاما\n",
            "\n",
            "Ground Truth: \tللحكام\n",
            "\n",
            "-\n",
            "Wrong sentence: ادناها\n",
            "Corrected sentence: أدناها\n",
            "\n",
            "Ground Truth: \tأدناها\n",
            "\n",
            "-\n",
            "Wrong sentence: .\n",
            "Corrected sentence: \n",
            "\n",
            "Ground Truth: \t!\n",
            "\n",
            "-\n",
            "Wrong sentence: كذالك\n",
            "Corrected sentence: كذلك\n",
            "\n",
            "Ground Truth: \tكذلك\n",
            "\n",
            "-\n",
            "Wrong sentence: اخواننا\n",
            "Corrected sentence: إخواننا\n",
            "\n",
            "Ground Truth: \tإخواننا\n",
            "\n",
            "-\n",
            "Wrong sentence: الصينينة\n",
            "Corrected sentence: الصينية\n",
            "\n",
            "Ground Truth: \tالصينية\n",
            "\n",
            "-\n",
            "Wrong sentence: ثلاتة\n",
            "Corrected sentence: ثلاثة\n",
            "\n",
            "Ground Truth: \tثلاثة\n",
            "\n",
            "-\n",
            "Wrong sentence: اشهر\n",
            "Corrected sentence: أشهر\n",
            "\n",
            "Ground Truth: \tأشهر\n",
            "\n",
            "-\n",
            "Wrong sentence: .\n",
            "Corrected sentence: \n",
            "\n",
            "Ground Truth: \t!\n",
            "\n",
            "-\n",
            "Wrong sentence: الاسى\n",
            "Corrected sentence: الأسي\n",
            "\n",
            "Ground Truth: \tالأسى\n",
            "\n",
            "-\n",
            "Wrong sentence: الاليم\n",
            "Corrected sentence: الأليم\n",
            "\n",
            "Ground Truth: \tالأليم\n",
            "\n",
            "-\n",
            "Wrong sentence: واطالب\n",
            "Corrected sentence: وأطالب\n",
            "\n",
            "Ground Truth: \tوأطالب\n",
            "\n",
            "-\n",
            "Wrong sentence: ان\n",
            "Corrected sentence: أن\n",
            "\n",
            "Ground Truth: \tأن\n",
            "\n",
            "-\n",
            "Wrong sentence: واسكنهم\n",
            "Corrected sentence: وأسكنهم\n",
            "\n",
            "Ground Truth: \tوأسكنهم\n",
            "\n",
            "-\n",
            "Wrong sentence: والهم\n",
            "Corrected sentence: وألهم\n",
            "\n",
            "Ground Truth: \tوألهم\n",
            "\n",
            "-\n",
            "Wrong sentence: ان\n",
            "Corrected sentence: أن\n",
            "\n",
            "Ground Truth: \tإنا\n",
            "\n",
            "-\n",
            "Wrong sentence: وان\n",
            "Corrected sentence: وإن\n",
            "\n",
            "Ground Truth: \tوإنا\n",
            "\n",
            "-\n",
            "Wrong sentence: اليه\n",
            "Corrected sentence: إليه\n",
            "\n",
            "Ground Truth: \tإليه\n",
            "\n",
            "-\n",
            "Wrong sentence: للاسف\n",
            "Corrected sentence: للأسف\n",
            "\n",
            "Ground Truth: \tللأسف\n",
            "\n",
            "-\n",
            "Wrong sentence: ان\n",
            "Corrected sentence: أن\n",
            "\n",
            "Ground Truth: \tإن\n",
            "\n",
            "-\n",
            "Wrong sentence: الاعلام\n",
            "Corrected sentence: الإعلام\n",
            "\n",
            "Ground Truth: \tالإعلام\n",
            "\n",
            "-\n",
            "Wrong sentence: اصبح\n",
            "Corrected sentence: أصبح\n",
            "\n",
            "Ground Truth: \tأصبح\n",
            "\n",
            "-\n",
            "Wrong sentence: واحساس\n",
            "Corrected sentence: وإحساس\n",
            "\n",
            "Ground Truth: \tوإحساس\n",
            "\n",
            "-\n",
            "Wrong sentence: وادناب\n",
            "Corrected sentence: وأذناب\n",
            "\n",
            "Ground Truth: \tوأذنابا\n",
            "\n",
            "-\n",
            "Wrong sentence: للامريكان\n",
            "Corrected sentence: للأمريكان\n",
            "\n",
            "Ground Truth: \tللأمريكان\n",
            "\n",
            "-\n",
            "Wrong sentence: احد\n",
            "Corrected sentence: أحد\n",
            "\n",
            "Ground Truth: \tأحد\n",
            "\n",
            "-\n",
            "Wrong sentence: اعتقد\n",
            "Corrected sentence: أعتقد\n",
            "\n",
            "Ground Truth: \tأعتقد\n",
            "\n",
            "-\n",
            "Wrong sentence: لافرق\n",
            "Corrected sentence: لا فرق\n",
            "\n",
            "Ground Truth: \tلا فرق\n",
            "\n",
            "-\n",
            "Wrong sentence: انها\n",
            "Corrected sentence: أنها\n",
            "\n",
            "Ground Truth: \tأنها\n",
            "\n",
            "-\n",
            "Wrong sentence: الامين\n",
            "Corrected sentence: الأمين\n",
            "\n",
            "Ground Truth: \tالأمين\n",
            "\n",
            "-\n",
            "Wrong sentence: اليس\n",
            "Corrected sentence: أليس\n",
            "\n",
            "Ground Truth: \tأليس\n",
            "\n",
            "-\n",
            "Wrong sentence: أنتصاراتهم\n",
            "Corrected sentence: انتصاراتهم\n",
            "\n",
            "Ground Truth: \tانتصاراتهم\n",
            "\n",
            "-\n",
            "Wrong sentence: وحررو\n",
            "Corrected sentence: وحرروا\n",
            "\n",
            "Ground Truth: \tوحرروا\n",
            "\n",
            "-\n",
            "Wrong sentence: الاسد\n",
            "Corrected sentence: الأسد\n",
            "\n",
            "Ground Truth: \tالأسد\n",
            "\n",
            "-\n",
            "Wrong sentence: منتهكآ\n",
            "Corrected sentence: منتهكا\n",
            "\n",
            "Ground Truth: \tمنتهكا\n",
            "\n",
            "-\n",
            "Wrong sentence: قررو\n",
            "Corrected sentence: قرروا\n",
            "\n",
            "Ground Truth: \tقرروا\n",
            "\n",
            "-\n",
            "Wrong sentence: القادم\n",
            "Corrected sentence: القادم\n",
            "\n",
            "Ground Truth: \tالقادمة\n",
            "\n",
            "-\n",
            "Wrong sentence: ما\n",
            "Corrected sentence: لا\n",
            "\n",
            "Ground Truth: \tمن\n",
            "\n",
            "-\n",
            "Wrong sentence: يدعو\n",
            "Corrected sentence: يدعون\n",
            "\n",
            "Ground Truth: \tيدعون\n",
            "\n",
            "-\n",
            "Wrong sentence: ان\n",
            "Corrected sentence: أن\n",
            "\n",
            "Ground Truth: \tأن\n",
            "\n",
            "-\n",
            "Wrong sentence: ارسلهم\n",
            "Corrected sentence: أرسلهم\n",
            "\n",
            "Ground Truth: \tأرسلهم\n",
            "\n",
            "-\n",
            "Wrong sentence: ايران\n",
            "Corrected sentence: إيران\n",
            "\n",
            "Ground Truth: \tإيران\n",
            "\n",
            "-\n",
            "Wrong sentence: أكتشفوا\n",
            "Corrected sentence: اكتشفوا\n",
            "\n",
            "Ground Truth: \tاكتشفوا\n",
            "\n",
            "-\n",
            "Wrong sentence: ان\n",
            "Corrected sentence: أن\n",
            "\n",
            "Ground Truth: \tأن\n",
            "\n",
            "-\n",
            "Wrong sentence: بتاييد\n",
            "Corrected sentence: بتأييد\n",
            "\n",
            "Ground Truth: \tبتأييد\n",
            "\n",
            "-\n",
            "Wrong sentence: وايضا\n",
            "Corrected sentence: وأيضا\n",
            "\n",
            "Ground Truth: \tوأيضا\n",
            "\n",
            "-\n",
            "Wrong sentence: ان\n",
            "Corrected sentence: أن\n",
            "\n",
            "Ground Truth: \tأن\n",
            "\n",
            "-\n",
            "Wrong sentence: والة\n",
            "Corrected sentence: وأله\n",
            "\n",
            "Ground Truth: \tوآلة\n",
            "\n",
            "-\n",
            "Wrong sentence: ان\n",
            "Corrected sentence: أن\n",
            "\n",
            "Ground Truth: \tإن\n",
            "\n",
            "-\n",
            "Wrong sentence: ان\n",
            "Corrected sentence: أن\n",
            "\n",
            "Ground Truth: \tأن\n",
            "\n",
            "-\n",
            "Wrong sentence: ميدفيدف\n",
            "Corrected sentence: ميدفيدا\n",
            "\n",
            "Ground Truth: \tميدفيديف\n",
            "\n",
            "-\n",
            "Wrong sentence: انه\n",
            "Corrected sentence: أنه\n",
            "\n",
            "Ground Truth: \tأنه\n",
            "\n",
            "-\n",
            "Wrong sentence: اطلاق\n",
            "Corrected sentence: إطلاق\n",
            "\n",
            "Ground Truth: \tإطلاق\n",
            "\n",
            "-\n",
            "Wrong sentence: اطلاق\n",
            "Corrected sentence: إطلاق\n",
            "\n",
            "Ground Truth: \tإطلاق\n",
            "\n",
            "-\n",
            "Wrong sentence: اطلاق\n",
            "Corrected sentence: إطلاق\n",
            "\n",
            "Ground Truth: \tإطلاق\n",
            "\n",
            "-\n",
            "Wrong sentence: اطلاق\n",
            "Corrected sentence: إطلاق\n",
            "\n",
            "Ground Truth: \tإطلاق\n",
            "\n",
            "-\n",
            "Wrong sentence: \n",
            "Corrected sentence: ،\n",
            "\n",
            "Ground Truth: \t\n",
            "\n",
            "-\n",
            "Wrong sentence: وعلي\n",
            "Corrected sentence: وعلى\n",
            "\n",
            "Ground Truth: \tوعلى\n",
            "\n",
            "-\n",
            "Wrong sentence: وليسوا\n",
            "Corrected sentence: وليس\n",
            "\n",
            "Ground Truth: \tوليس\n",
            "\n",
            "-\n",
            "Wrong sentence: إحتراما\n",
            "Corrected sentence: احتراما\n",
            "\n",
            "Ground Truth: \tاحتراما\n",
            "\n",
            "-\n",
            "Wrong sentence: لايقوم\n",
            "Corrected sentence: لا يقوم\n",
            "\n",
            "Ground Truth: \tلا يقوم\n",
            "\n",
            "-\n",
            "Wrong sentence: إ\n",
            "Corrected sentence: \n",
            "\n",
            "Ground Truth: \t\n",
            "\n",
            "-\n",
            "Wrong sentence: لايستخدم\n",
            "Corrected sentence: لا يستخدم\n",
            "\n",
            "Ground Truth: \tلا يستخدم\n",
            "\n",
            "-\n",
            "Wrong sentence: ارض\n",
            "Corrected sentence: أرض\n",
            "\n",
            "Ground Truth: \tأرض\n",
            "\n",
            "-\n",
            "Wrong sentence: ان\n",
            "Corrected sentence: أن\n",
            "\n",
            "Ground Truth: \tأن\n",
            "\n",
            "-\n",
            "Wrong sentence: اجانب\n",
            "Corrected sentence: أجانب\n",
            "\n",
            "Ground Truth: \tأجانب\n",
            "\n",
            "-\n",
            "Wrong sentence: لانها\n",
            "Corrected sentence: لأنها\n",
            "\n",
            "Ground Truth: \tلأنها\n",
            "\n",
            "-\n",
            "Wrong sentence: الا\n",
            "Corrected sentence: إلا\n",
            "\n",
            "Ground Truth: \tإلا\n",
            "\n",
            "-\n",
            "Wrong sentence: لان\n",
            "Corrected sentence: لأن\n",
            "\n",
            "Ground Truth: \tلأن\n",
            "\n",
            "-\n",
            "Wrong sentence: لايقبل\n",
            "Corrected sentence: لا يقبل\n",
            "\n",
            "Ground Truth: \tلا يقبل\n",
            "\n",
            "-\n",
            "Wrong sentence: يتسائلون\n",
            "Corrected sentence: يتسائلون\n",
            "\n",
            "Ground Truth: \tيتساءلون\n",
            "\n",
            "-\n",
            "Wrong sentence: يحضوا\n",
            "Corrected sentence: يحضووا\n",
            "\n",
            "Ground Truth: \tيحرضوا\n",
            "\n",
            "-\n",
            "Wrong sentence: الاسف\n",
            "Corrected sentence: الأسف\n",
            "\n",
            "Ground Truth: \tالأسف\n",
            "\n",
            "-\n",
            "Wrong sentence: ماهم\n",
            "Corrected sentence: ما هم\n",
            "\n",
            "Ground Truth: \tما هم\n",
            "\n",
            "-\n",
            "Wrong sentence: الان\n",
            "Corrected sentence: الآن\n",
            "\n",
            "Ground Truth: \tالآن\n",
            "\n",
            "-\n",
            "Wrong sentence: لاسباب\n",
            "Corrected sentence: لأسباب\n",
            "\n",
            "Ground Truth: \tلأسباب\n",
            "\n",
            "-\n",
            "Wrong sentence: تائفة\n",
            "Corrected sentence: تائفة\n",
            "\n",
            "Ground Truth: \tتافهة\n",
            "\n",
            "-\n",
            "Wrong sentence: ان\n",
            "Corrected sentence: أن\n",
            "\n",
            "Ground Truth: \tأن\n",
            "\n",
            "-\n",
            "Wrong sentence: اخوة\n",
            "Corrected sentence: إخوة\n",
            "\n",
            "Ground Truth: \tإخوة\n",
            "\n",
            "-\n",
            "Wrong sentence: امن\n",
            "Corrected sentence: أمن\n",
            "\n",
            "Ground Truth: \tأمن\n",
            "\n",
            "-\n",
            "Wrong sentence: الاف\n",
            "Corrected sentence: آلاف\n",
            "\n",
            "Ground Truth: \tآلاف\n",
            "\n",
            "-\n",
            "Wrong sentence: انا\n",
            "Corrected sentence: أنا\n",
            "\n",
            "Ground Truth: \tأنا\n",
            "\n",
            "-\n",
            "Wrong sentence: او\n",
            "Corrected sentence: أو\n",
            "\n",
            "Ground Truth: \tأو\n",
            "\n",
            "-\n",
            "Wrong sentence: او\n",
            "Corrected sentence: أو\n",
            "\n",
            "Ground Truth: \tأو\n",
            "\n",
            "-\n",
            "Wrong sentence: الخ\n",
            "Corrected sentence: إلخ\n",
            "\n",
            "Ground Truth: \tإلخ\n",
            "\n",
            "-\n",
            "Wrong sentence: ولافرق\n",
            "Corrected sentence: ولافر\n",
            "\n",
            "Ground Truth: \tولا فرق\n",
            "\n",
            "-\n",
            "Wrong sentence: الا\n",
            "Corrected sentence: إلا\n",
            "\n",
            "Ground Truth: \tإلا\n",
            "\n",
            "-\n",
            "Wrong sentence: يفول\n",
            "Corrected sentence: يقول\n",
            "\n",
            "Ground Truth: \tيقول\n",
            "\n",
            "-\n",
            "Wrong sentence: امرءأ\n",
            "Corrected sentence: امرأ\n",
            "\n",
            "Ground Truth: \tامرئا\n",
            "\n",
            "-\n",
            "Wrong sentence: اليسوا\n",
            "Corrected sentence: أليسوا\n",
            "\n",
            "Ground Truth: \tأليسوا\n",
            "\n",
            "-\n",
            "Wrong sentence: اهل\n",
            "Corrected sentence: أهل\n",
            "\n",
            "Ground Truth: \tأهل\n",
            "\n",
            "-\n",
            "Wrong sentence: السنه\n",
            "Corrected sentence: السنة\n",
            "\n",
            "Ground Truth: \tالسنة\n",
            "\n",
            "-\n",
            "Wrong sentence: ارسلوا\n",
            "Corrected sentence: أرسلوا\n",
            "\n",
            "Ground Truth: \tأرسلوا\n",
            "\n",
            "-\n",
            "Wrong sentence: اثنان\n",
            "Corrected sentence: اثنان\n",
            "\n",
            "Ground Truth: \tاثنين\n",
            "\n",
            "-\n",
            "Wrong sentence: السنه\n",
            "Corrected sentence: السنة\n",
            "\n",
            "Ground Truth: \tالسنة\n",
            "\n",
            "-\n",
            "Wrong sentence: مايقارب\n",
            "Corrected sentence: ما يقارب\n",
            "\n",
            "Ground Truth: \tما يقارب\n",
            "\n",
            "-\n",
            "Wrong sentence: واعاقوا\n",
            "Corrected sentence: وأعاقوا\n",
            "\n",
            "Ground Truth: \tوأعاقوا\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqCtSC1tBpIQ"
      },
      "source": [
        "# **Word Error Rate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf6j2Lq2-8By"
      },
      "source": [
        "!pip install jiwer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24rWSVqpEm7R"
      },
      "source": [
        "hypothesis=[]\n",
        "ground_truth=[]\n",
        "for seq_index in range(len(test_x)):\n",
        "    input_seq = encoderInput_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence=decode_sequence(input_seq)\n",
        "    hypothesis.append(decoded_sentence)\n",
        "    ground_truth.append(target_sents[seq_index])"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2vZXBhSpCFe"
      },
      "source": [
        "from jiwer import wer\n",
        "import jiwer\n",
        "transformation = jiwer.Compose([\n",
        "    jiwer.ToLowerCase(),\n",
        "    jiwer.RemoveMultipleSpaces(),\n",
        "    jiwer.RemoveWhiteSpace(replace_by_space=False),\n",
        "    jiwer.SentencesToListOfWords(word_delimiter=\" \")\n",
        "]) \n",
        "error = wer(ground_truth, hypothesis,truth_transform=transformation,hypothesis_transform=transformation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS5hk1NzCCxT",
        "outputId": "f37f4e45-c5a8-4a99-ed96-41a06cc260e6"
      },
      "source": [
        "print(\"Word Error Rate : \",error)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Error Rate :  0.24087932647333957\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}